{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# AiED: Artificial intelligence for the detection of intracranial interictal epileptiform discharges\n",
    "### Dartmouth ECoG Lab \n",
    "#### version 1 (2021)\n",
    "#### version 2 (2024) - CC edits\n",
    "#### version 3 (2024) - PB edits\n",
    "- added .windows to prevent import error: template = signal.windows.triang(np.round(down_samp_freq * 0.06))\n",
    "- reformated exported so that channel index list is in one column and channel name list is in another column\n",
    "- added Camilo's tuple fix\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "from __future__ import print_function, division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "import os\n",
    "import h5py\n",
    "import re\n",
    "import shutil\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import warnings \n",
    "import operator\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from matplotlib.pyplot import specgram\n",
    "import torch\n",
    "#import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "#from torchtext import data\n",
    "from sklearn.metrics import precision_score, f1_score, recall_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "import joblib\n",
    "import csv\n",
    "import warnings\n",
    "import glob\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHECK: Number of channels ~ 143\n"
     ]
    }
   ],
   "source": [
    "### 1. LOAD DATA: rows = channels, cols = timepoints\n",
    "subject = 'Mus21' ################################################## CHANGE: SUBJECT NAME\n",
    "run = \"1\" ################################################## CHANGE: RUN #\n",
    "\n",
    "project_dir = re.sub(\"/Code/Spike detection\", \"\", os.getcwd())\n",
    "input_eeg_file = glob.glob(f\"{project_dir}/Data/Preprocessed/{subject}_run{run}_*.csv\")[0]  # glob.glob finds file based on partial string match - wildcard for differing date\n",
    "\n",
    "\n",
    "with open(input_eeg_file, 'rb') as csvfile:\n",
    "    csv_test_bytes = csvfile.read(10)  # grab sample of .csv for format detection\n",
    "    headertest = csv_test_bytes.decode(\"utf-8\")\n",
    "    if any(c.isalpha() for c in headertest) == True:\n",
    "        data = pd.read_csv(input_eeg_file, header=0)\n",
    "        channels = data.columns\n",
    "    else:\n",
    "        data = pd.read_csv(input_eeg_file, header=None)\n",
    "        \n",
    "### quick check: transpose if not in proper format (rows = chans, cols = timepoints) - build on this later.\n",
    "if len(data) > len(data.columns):\n",
    "    data = data.T\n",
    "    if type(data[0][0]) == str:\n",
    "        data = data.drop(data.columns[0], axis=1)\n",
    "    data = data.astype(float)\n",
    "    print('CHECK: Number of channels ~ %d' % len(data))\n",
    "else:\n",
    "    data = data.astype(float)\n",
    "\n",
    "    \n",
    "### AUTO DUMP IED IMAGES: clears dir containing spectrograms if produced in previous iteration\n",
    "spectdir = project_dir + '/Graphs/SPECTS/IEDS/' ################################################################## CHANGE: dir here\n",
    "os.makedirs(spectdir, exist_ok = True)\n",
    "for filename in os.listdir(spectdir):\n",
    "    file_path = os.path.join(spectdir, filename)\n",
    "    try:\n",
    "        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "            os.unlink(file_path)\n",
    "        elif os.path.isdir(file_path):\n",
    "            shutil.rmtree(file_path)\n",
    "    except Exception as e:\n",
    "        print('Failed to delete %s. Reason: %s' % (file_path, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['# LTA1', 'LTA2', 'LTA3', 'LTA4', 'LTA5', 'LTA6', 'LTA8', 'LTA9',\n",
      "       'LTA10', 'LTA11',\n",
      "       ...\n",
      "       'RFCA7', 'RFCA8', 'RFCA9', 'RFCA10', 'RFCA11', 'RFCA12', 'RFCA13',\n",
      "       'RFCA14', 'RFCA15', 'RFCA16'],\n",
      "      dtype='object', length=143)\n"
     ]
    }
   ],
   "source": [
    "print(channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th># LTA1</th>\n",
       "      <td>-6.454156e-10</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>5.354544e-05</td>\n",
       "      <td>4.822285e-05</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>4.350122e-05</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LTA2</th>\n",
       "      <td>-1.304294e-09</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>1.001232e-04</td>\n",
       "      <td>9.094074e-05</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>8.477089e-05</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LTA3</th>\n",
       "      <td>-1.693821e-09</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>1.368249e-04</td>\n",
       "      <td>1.243594e-04</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>1.143930e-04</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LTA4</th>\n",
       "      <td>-2.126791e-09</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>1.744966e-04</td>\n",
       "      <td>1.593398e-04</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>1.427856e-04</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LTA5</th>\n",
       "      <td>-2.452417e-09</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>2.103791e-04</td>\n",
       "      <td>1.952118e-04</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>1.724036e-04</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RFCA12</th>\n",
       "      <td>-1.614434e-09</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-2.975879e-05</td>\n",
       "      <td>-2.125587e-05</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>5.900842e-07</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RFCA13</th>\n",
       "      <td>-2.676450e-10</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-9.056368e-06</td>\n",
       "      <td>-1.112888e-05</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-5.426273e-06</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RFCA14</th>\n",
       "      <td>-1.896592e-10</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-9.753594e-06</td>\n",
       "      <td>-1.232715e-05</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-1.754939e-05</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RFCA15</th>\n",
       "      <td>-9.296550e-10</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-1.045848e-05</td>\n",
       "      <td>-1.896804e-05</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-3.325425e-05</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>-0.000045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RFCA16</th>\n",
       "      <td>1.120193e-09</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-2.481218e-07</td>\n",
       "      <td>-9.955095e-07</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-2.965969e-05</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0         1         2         3         4         5    \\\n",
       "# LTA1 -6.454156e-10  0.000043  0.000070  0.000103  0.000129  0.000128   \n",
       "LTA2   -1.304294e-09  0.000036  0.000068  0.000098  0.000124  0.000128   \n",
       "LTA3   -1.693821e-09  0.000036  0.000071  0.000100  0.000126  0.000134   \n",
       "LTA4   -2.126791e-09  0.000035  0.000072  0.000102  0.000131  0.000141   \n",
       "LTA5   -2.452417e-09  0.000033  0.000071  0.000103  0.000133  0.000141   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "RFCA12 -1.614434e-09  0.000013  0.000032  0.000038  0.000039  0.000018   \n",
       "RFCA13 -2.676450e-10  0.000013  0.000028  0.000034  0.000028 -0.000004   \n",
       "RFCA14 -1.896592e-10  0.000017  0.000029  0.000028  0.000020 -0.000004   \n",
       "RFCA15 -9.296550e-10  0.000009  0.000009  0.000009  0.000003 -0.000008   \n",
       "RFCA16  1.120193e-09  0.000013  0.000023  0.000026  0.000020  0.000004   \n",
       "\n",
       "             6         7         8         9    ...       91            92   \\\n",
       "# LTA1  0.000135  0.000160  0.000184  0.000210  ...  0.000047  5.354544e-05   \n",
       "LTA2    0.000133  0.000155  0.000172  0.000194  ...  0.000098  1.001232e-04   \n",
       "LTA3    0.000144  0.000164  0.000179  0.000200  ...  0.000136  1.368249e-04   \n",
       "LTA4    0.000154  0.000173  0.000187  0.000209  ...  0.000173  1.744966e-04   \n",
       "LTA5    0.000155  0.000175  0.000191  0.000214  ...  0.000207  2.103791e-04   \n",
       "...          ...       ...       ...       ...  ...       ...           ...   \n",
       "RFCA12  0.000002 -0.000003 -0.000016 -0.000012  ... -0.000033 -2.975879e-05   \n",
       "RFCA13 -0.000016 -0.000009 -0.000018 -0.000019  ... -0.000016 -9.056368e-06   \n",
       "RFCA14 -0.000018 -0.000026 -0.000048 -0.000048  ... -0.000004 -9.753594e-06   \n",
       "RFCA15 -0.000014 -0.000016 -0.000022 -0.000014  ... -0.000007 -1.045848e-05   \n",
       "RFCA16 -0.000004 -0.000011 -0.000015  0.000008  ... -0.000007 -2.481218e-07   \n",
       "\n",
       "                 93        94        95            96        97        98   \\\n",
       "# LTA1  4.822285e-05  0.000054  0.000048  4.350122e-05  0.000040  0.000035   \n",
       "LTA2    9.094074e-05  0.000095  0.000092  8.477089e-05  0.000078  0.000079   \n",
       "LTA3    1.243594e-04  0.000126  0.000123  1.143930e-04  0.000108  0.000109   \n",
       "LTA4    1.593398e-04  0.000159  0.000154  1.427856e-04  0.000137  0.000137   \n",
       "LTA5    1.952118e-04  0.000190  0.000185  1.724036e-04  0.000169  0.000169   \n",
       "...              ...       ...       ...           ...       ...       ...   \n",
       "RFCA12 -2.125587e-05 -0.000016 -0.000004  5.900842e-07 -0.000004 -0.000010   \n",
       "RFCA13 -1.112888e-05 -0.000011 -0.000007 -5.426273e-06 -0.000005 -0.000007   \n",
       "RFCA14 -1.232715e-05 -0.000012 -0.000016 -1.754939e-05 -0.000019 -0.000022   \n",
       "RFCA15 -1.896804e-05 -0.000024 -0.000030 -3.325425e-05 -0.000038 -0.000039   \n",
       "RFCA16 -9.955095e-07 -0.000005 -0.000015 -2.965969e-05 -0.000033 -0.000031   \n",
       "\n",
       "             99        100  \n",
       "# LTA1  0.000030  0.000032  \n",
       "LTA2    0.000076  0.000075  \n",
       "LTA3    0.000099  0.000095  \n",
       "LTA4    0.000124  0.000116  \n",
       "LTA5    0.000155  0.000143  \n",
       "...          ...       ...  \n",
       "RFCA12 -0.000020 -0.000034  \n",
       "RFCA13 -0.000018 -0.000023  \n",
       "RFCA14 -0.000029 -0.000041  \n",
       "RFCA15 -0.000041 -0.000045  \n",
       "RFCA16 -0.000040 -0.000050  \n",
       "\n",
       "[143 rows x 101 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[:,0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. LOAD TEMPLATE-MATCHING DETECTOR FUNCTIONS:\n",
    "\n",
    "def detect_peaks(x, mph=None, mpd=1, threshold=0, edge='rising',\n",
    "                kpsh=False, valley=False, show=False, ax=None):\n",
    "    \"\"\"Detect peaks in data based on their amplitude and other features.\"\"\"\n",
    "    x = np.atleast_1d(x).astype('float64')\n",
    "    if x.size < 3:\n",
    "        return np.array([], dtype=int)\n",
    "    if valley:\n",
    "        x = -x\n",
    "    # find indices of all peaks\n",
    "    dx = x[1:] - x[:-1]\n",
    "    indnan = np.where(np.isnan(x))[0]\n",
    "    if indnan.size:\n",
    "        x[indnan] = np.inf\n",
    "        dx[np.where(np.isnan(dx))[0]] = np.inf\n",
    "    ine, ire, ife = np.array([[], [], []], dtype=int)\n",
    "    if not edge:\n",
    "        ine = np.where((np.hstack((dx, 0)) < 0) & (np.hstack((0, dx)) > 0))[0]\n",
    "    else:\n",
    "        if edge.lower() in ['rising', 'both']:\n",
    "            ire = np.where((np.hstack((dx, 0)) <= 0) & (np.hstack((0, dx)) > 0))[0]\n",
    "        if edge.lower() in ['falling', 'both']:\n",
    "            ife = np.where((np.hstack((dx, 0)) < 0) & (np.hstack((0, dx)) >= 0))[0]\n",
    "    ind = np.unique(np.hstack((ine, ire, ife)))\n",
    "    if ind.size and indnan.size:\n",
    "        # NaN's and values close to NaN's cannot be peaks\n",
    "        ind = ind[np.in1d(ind, np.unique(np.hstack((indnan, indnan-1, indnan+1))), invert=True)]\n",
    "    # first and last values of x cannot be peaks\n",
    "    if ind.size and ind[0] == 0:\n",
    "        ind = ind[1:]\n",
    "    if ind.size and ind[-1] == x.size-1:\n",
    "        ind = ind[:-1]\n",
    "    # remove peaks < minimum peak height\n",
    "    if ind.size and mph is not None:\n",
    "        ind = ind[x[ind] >= mph]\n",
    "    # remove peaks - neighbors < threshold\n",
    "    if ind.size and threshold > 0:\n",
    "        dx = np.min(np.vstack([x[ind]-x[ind-1], x[ind]-x[ind+1]]), axis=0)\n",
    "        ind = np.delete(ind, np.where(dx < threshold)[0])\n",
    "    # detect small peaks closer than minimum peak distance\n",
    "    if ind.size and mpd > 1:\n",
    "        ind = ind[np.argsort(x[ind])][::-1]  # sort ind by peak height\n",
    "        idel = np.zeros(ind.size, dtype=bool)\n",
    "        for i in range(ind.size):\n",
    "            if not idel[i]:\n",
    "                # keep peaks with the same height if kpsh is True\n",
    "                idel = idel | (ind >= ind[i] - mpd) & (ind <= ind[i] + mpd) \\\n",
    "                    & (x[ind[i]] > x[ind] if kpsh else True)\n",
    "                idel[i] = 0  # Keep current peak\n",
    "        # remove the small peaks and sort back the indices by their occurrence\n",
    "        ind = np.sort(ind[~idel])\n",
    "\n",
    "    if show:\n",
    "        if indnan.size:\n",
    "            x[indnan] = np.nan\n",
    "        if valley:\n",
    "            x = -x\n",
    "        _plot(x, mph, mpd, threshold, edge, valley, ax, ind)\n",
    "    return ind\n",
    "\n",
    "def _plot(x, mph, mpd, threshold, edge, valley, ax, ind):\n",
    "    \"\"\"Plot results of the detect_peaks function, see its help.\"\"\"\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "    except ImportError:\n",
    "        print('matplotlib is not available.')\n",
    "    else:\n",
    "        if ax is None:\n",
    "            _, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "\n",
    "        ax.plot(x, 'b', lw=1)\n",
    "        if ind.size:\n",
    "            label = 'valley' if valley else 'peak'\n",
    "            label = label + 's' if ind.size > 1 else label\n",
    "            ax.plot(ind, x[ind], '+', mfc=None, mec='r', mew=2, ms=8,\n",
    "                    label='%d %s' % (ind.size, label))\n",
    "            ax.legend(loc='best', framealpha=.5, numpoints=1)\n",
    "        ax.set_xlim(-.02*x.size, x.size*1.02-1)\n",
    "        ymin, ymax = x[np.isfinite(x)].min(), x[np.isfinite(x)].max()\n",
    "        yrange = ymax - ymin if ymax > ymin else 1\n",
    "        ax.set_ylim(ymin - 0.1*yrange, ymax + 0.1*yrange)\n",
    "        ax.set_xlabel('Data #', fontsize=14)\n",
    "        ax.set_ylabel('Amplitude', fontsize=14)\n",
    "        mode = 'Valley detection' if valley else 'Peak detection'\n",
    "        ax.set_title(\"%s (mph=%s, mpd=%d, threshold=%s, edge='%s')\"\n",
    "                    % (mode, str(mph), mpd, str(threshold), edge))\n",
    "        # plt.grid()\n",
    "        plt.show()\n",
    "        \n",
    "def locate_downsample_freq(sample_freq, min_freq=200, max_freq=340):\n",
    "    min_up_factor = np.inf\n",
    "    best_candidate_freq = None\n",
    "    for candidate in range(min_freq, max_freq+1):\n",
    "        down_samp_rate = sample_freq / float(candidate)\n",
    "        down_factor, up_factor = down_samp_rate.as_integer_ratio()\n",
    "        if up_factor <= min_up_factor:\n",
    "            min_up_factor = up_factor\n",
    "            best_candidate_freq = candidate\n",
    "    return best_candidate_freq\n",
    "\n",
    "\n",
    "def butter_bandpass(low_limit, high_limit, samp_freq, order=5):\n",
    "    nyquist_limit = samp_freq / 2\n",
    "    low_prop = low_limit / nyquist_limit\n",
    "    high_prop = high_limit / nyquist_limit\n",
    "    b, a = signal.butter(order, [low_prop, high_prop], btype='band')\n",
    "    def bb_filter(data):\n",
    "        return signal.filtfilt(b, a, data)\n",
    "    return bb_filter\n",
    "\n",
    "\n",
    "def detect(channel, samp_freq, return_eeg=False, temp_func=None, signal_func=None):\n",
    "    # assume that eeg is [channels x samples]\n",
    "    # Round samp_freq to the nearest integer if it is large\n",
    "    if samp_freq > 100:\n",
    "        samp_freq = int(np.round(samp_freq))\n",
    "    down_samp_freq = locate_downsample_freq(samp_freq)\n",
    "    template = signal.windows.triang(np.round(down_samp_freq * 0.06))\n",
    "    kernel = np.array([-2, -1, 1, 2]) / float(8)\n",
    "    template = np.convolve(kernel, np.convolve(template, kernel, 'valid') ,'full')\n",
    "    if temp_func:\n",
    "        template = temp_func(template, samp_freq)\n",
    "    if signal_func:\n",
    "        channel = signal_func(channel, samp_freq)\n",
    "\n",
    "    down_samp_rate = samp_freq / float(down_samp_freq)\n",
    "    down_samp_factor, up_samp_factor = down_samp_rate.as_integer_ratio()\n",
    "    channel = signal.detrend(channel, type='constant')\n",
    "    results = template_match(channel, template, down_samp_freq)\n",
    "    up_samp_results = [np.round(spikes * down_samp_factor / float(up_samp_factor)).astype(int) for spikes in results]\n",
    "    if return_eeg:\n",
    "        return up_samp_results, [channel[start:end] for start, end in results]\n",
    "    else:\n",
    "        return up_samp_results\n",
    "\n",
    "def template_match(channel, template, down_samp_freq, thresh=7, min_spacing=0): #######@@@############################## CHANGE: d:7,0\n",
    "    template_len = len(template)\n",
    "    cross_corr = np.convolve(channel, template, 'valid')\n",
    "    cross_corr_std = med_std(cross_corr, down_samp_freq)\n",
    "    detections = []\n",
    "    # catch empty channels\n",
    "    if cross_corr_std > 0:\n",
    "        # normalize the cross-correlation\n",
    "        cross_corr_norm = ((cross_corr - np.mean(cross_corr)) / cross_corr_std)\n",
    "        cross_corr_norm[1] = 0\n",
    "        cross_corr_norm[-1] = 0\n",
    "        # find regions with high cross-corr\n",
    "        if np.any(abs(cross_corr_norm > thresh)):\n",
    "            peaks = detect_peaks(abs(cross_corr_norm), mph=thresh, mpd=template_len)\n",
    "            peaks += int(np.ceil(template_len / 2.)) # center detection on template\n",
    "            peaks = [peak for peak in peaks if peak > template_len and peak <= len(channel)-template_len]\n",
    "            if peaks:\n",
    "                # find peaks that are at least (min_spacing) secs away\n",
    "                distant_peaks = np.diff(peaks) > min_spacing * down_samp_freq\n",
    "                # always keep the first peak\n",
    "                to_keep = np.insert(distant_peaks, 0, True)\n",
    "                peaks = [peaks[x] for x in range(len(peaks)) if to_keep[x] == True]\n",
    "                detections = [(peak-template_len, peak+template_len) for peak in peaks]\n",
    "    return np.array(detections)\n",
    "\n",
    "def med_std(signal, window_len):\n",
    "    window = np.zeros(window_len) + (1 / float(window_len))\n",
    "    std = np.sqrt(np.median(np.convolve(np.square(signal), window, 'valid') - np.square(np.convolve(signal, window, 'valid'))))\n",
    "    return std\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPIKES DETECTED (TEMP MATCH) =  88353\n",
      "\n",
      "   channel     spikeTime   fs subject\n",
      "0        0  [1415, 1439]  200   Mus21\n",
      "1        0  [2384, 2408]  200   Mus21\n",
      "2        0  [2600, 2624]  200   Mus21\n"
     ]
    }
   ],
   "source": [
    "### 3. RUN TEMPLATE-MATCHING DETECTOR:\n",
    "def autoDetect(eegdata, samp_freq = 200, subject = subject):\n",
    "    \"\"\"\n",
    "    AUTODETECT: DETECTS ALL SPIKES IN EACH CHANNEL\n",
    "         INPUT: raw eeg file (preprocessed signal)\n",
    "        OUTPUT: all_detections (list containing a list of arrays for all detections), \n",
    "                channel_names (eeg channel names corresponding to each detection list)\n",
    "    \"\"\"\n",
    "    ### DETECT SPIKES:\n",
    "    all_detections = []\n",
    "    channel_names = []\n",
    "    for i in range(eegdata.shape[0]):\n",
    "        channel = eegdata.iloc[i,:].astype(float) # run on each row (chan)\n",
    "        detections = detect(channel, samp_freq, return_eeg=False, temp_func=None, signal_func=None) \n",
    "        all_detections.append(detections)\n",
    "        channel_names.append(int(float((eegdata.columns[i]))))\n",
    "\n",
    "    ### REFORMAT SPIKES:\n",
    "    detections = pd.DataFrame(all_detections)\n",
    "    channels = pd.DataFrame(channel_names)\n",
    "    spikes = pd.concat([channels,detections], axis = 1)\n",
    "    newspikes = spikes.transpose() \n",
    "    newspikes.columns = newspikes.iloc[0]\n",
    "    newspikes = newspikes.iloc[1:] # remove duplicate channel_name row \n",
    "    ### AUTO LONG-FORMATTING OF SPIKES\n",
    "    spikeDf = pd.DataFrame() # empty df to store final spikes and spikeTimes \n",
    "    for idx, col in enumerate(newspikes.columns):\n",
    "        # extract spikes for each column \n",
    "        tempSpikes = newspikes.iloc[:,idx].dropna() # column corresponding to channel with all spikes\n",
    "        tempSpikes2 = tempSpikes.tolist() # convert series to list \n",
    "        # extract channel name for each spike (duplicate based on the number of spikes)\n",
    "        tempName = tempSpikes.name # channel name \n",
    "        tempName2 = [tempName] * len(tempSpikes) # repeat col name by the number of spikes in this channel \n",
    "        tempDf = pd.DataFrame({'channel': tempName2, 'spikeTime': tempSpikes2})\n",
    "        # save and append to final df \n",
    "        #spikeDf = spikeDf.append(tempDf) #ORIGINAL\n",
    "        #spikeDf = pd.merge(spikeDf,tempDf)\n",
    "        spikeDf = pd.concat([spikeDf, tempDf])\n",
    "        spikeDf['fs'] = samp_freq\n",
    "        spikeDf['subject'] = subject\n",
    "    return(spikeDf)\n",
    "\n",
    "#https://stackoverflow.com/questions/75956209/error-dataframe-object-has-no-attribute-append\n",
    "spikes = autoDetect(data) ### eegfile, Fs, sessionname; kleen_fs=200, preprocess_fs=200\n",
    "\n",
    "print(\"SPIKES DETECTED (TEMP MATCH) = \", len(spikes))\n",
    "print(\"\")\n",
    "print(spikes[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/88353 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88353/88353 [5:01:40<00:00,  4.88it/s]      \n"
     ]
    }
   ],
   "source": [
    "### 4. GENERATE INPUT IMAGES FOR CNN:\n",
    "def spectimgs(eegdata, spikedf):\n",
    "    \"\"\"\n",
    "    SPECTS: GENERATE SPECTS FOR CNN\n",
    "        INPUT: 1) eegdata, 2) spikedf (df from automated template-matching spike detector)\n",
    "        OUTPUT: spects within ./SPECTS/IEDS\n",
    "    \"\"\"\n",
    "    for i in tqdm(range(0,len(spikedf))): \n",
    "        samp_freq = int(float(spikedf.fs.values[0]))\n",
    "        #######################################\n",
    "        pad = 1 # d:1 number of seconds for window \n",
    "        dpi_setting = 300 # d:300\n",
    "        Nfft = 128*(samp_freq/500) # d: 128 \n",
    "        h = 3\n",
    "        w = 3\n",
    "        #######################################\n",
    "        try:\n",
    "            subject = spikedf.subject.values[0]\n",
    "            chan_name = int(spikedf.channel.values[i]) # zero idxed -1\n",
    "            spikestart = spikedf.spikeTime.values[i][0] # start spike\n",
    "            ### select eeg data row \n",
    "            ecogclip = eegdata.iloc[chan_name]\n",
    "            ### filter out line noise\n",
    "            b_notch, a_notch = signal.iirnotch(60.0, 30.0, samp_freq)\n",
    "            ecogclip = pd.Series(signal.filtfilt(b_notch, a_notch, ecogclip)) \n",
    "        \n",
    "            ### trim eeg clip based on cushion            \n",
    "            ### mean imputation if missing indices\n",
    "            end = int(float((spikestart+int(float(pad*samp_freq)))))\n",
    "            start = int(float((spikestart-int(float(pad*samp_freq)))))\n",
    "            if end > max(ecogclip.index):\n",
    "                temp = list(ecogclip[list(range(spikestart-int(float(pad*samp_freq)), max(ecogclip.index)))])\n",
    "                cushend = [np.mean(ecogclip)]*(end - max(ecogclip.index))\n",
    "                temp = np.array(temp + cushend)\n",
    "            elif start < min(ecogclip.index):\n",
    "                temp = list(ecogclip[list(range(min(ecogclip.index), spikestart+pad*samp_freq))])\n",
    "                cushstart = [np.mean(ecogclip)]*(min(ecogclip.index)-start)\n",
    "                temp = np.array(cushstart + temp)\n",
    "                ## CC additions:\n",
    "                #temp = np.array(cushstart, temp) #(,) changed for +\n",
    "                #If this doesn't work we can always do:\n",
    "                #array_cushion = np.array(cushstart)\n",
    "                #array_temp = np.array(temp)\n",
    "                #temp = np.hstack((array_cushion,array_temp))\n",
    "            else:\n",
    "                temp = np.array(ecogclip[list(range(spikestart-int(float(pad*samp_freq)), \n",
    "                                         spikestart+int(float(pad*samp_freq))))]) \n",
    "           \n",
    "            \n",
    "            ### PLOT AND EXPORT:\n",
    "            plt.figure(figsize=(h,w))\n",
    "            specgram(temp, NFFT = int(Nfft), Fs = samp_freq, noverlap=int(Nfft/2), detrend = \"linear\", cmap = \"YlOrRd\") \n",
    "            plt.axis(\"off\")\n",
    "            plt.xlim(0, pad*2)\n",
    "            plt.ylim(0,100)\n",
    "            plt.savefig(spectdir+subject+\"_\"+str(spikestart)+\"_\"+str(chan_name)+\".png\", dpi = dpi_setting)\n",
    "            plt.close()\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "            print(\"ERROR with IED portion:\", i)\n",
    "            plt.close()\n",
    "            continue\n",
    "\n",
    "spectimgs(data, spikes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5. ResNet-18 CNN DETECTOR:\n",
    "### A: LOAD ALL DATA --- extract clip_id from path\n",
    "model_dir = f\"{project_dir}/Code/\" # dir with trained model ################################################## CHANGE: dir here\n",
    "imgs = f'{project_dir}/Graphs/SPECTS'\n",
    "\n",
    "data_transforms = {\n",
    "    imgs: transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.Pad(1, fill=0, padding_mode='constant'),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])}\n",
    "\n",
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "    \"\"\"Custom dataset that includes image file paths.\n",
    "    Extends torchvision.datasets.ImageFolder\n",
    "    \"\"\"\n",
    "    # override the __getitem__ method. this is the method that dataloader calls\n",
    "    def __getitem__(self, index):\n",
    "        # this is what ImageFolder normally returns \n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        # the image file path\n",
    "        path = self.imgs[index][0]\n",
    "        # make a new tuple that includes original and the path\n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "        return (tuple_with_path)\n",
    "    \n",
    "image_datasets = {x: ImageFolderWithPaths(os.path.join(project_dir, x),\n",
    "                                          data_transforms[x]) for x in [imgs]}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=1, # use batch=1, shuffle=F\n",
    "                                             shuffle=False, num_workers=0) for x in [imgs]} \n",
    "class_names = image_datasets[imgs].classes\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# extract image paths\n",
    "path_names = []\n",
    "for images,labels,paths in dataloaders[imgs]:\n",
    "    path_names.append(paths)\n",
    "# convert list of paths to dataframe col\n",
    "df = pd.DataFrame(path_names)\n",
    "df.columns = ['clip_ids']\n",
    "df[['clip_ids','clip']] = df['clip_ids'].str.split('IEDS/',expand=True)\n",
    "df['clip'] = df['clip'].str.rstrip('.png')\n",
    "df[['subject','start','chan']] = df['clip'].str.split('_',expand=True)\n",
    "\n",
    "##############################################\n",
    "\n",
    "### B: LOAD PRETRAINED MODEL \n",
    "try: \n",
    "    model = torch.load(model_dir+'model_aied.pt')\n",
    "    # model.eval() # model architecture\n",
    "except ImportError:\n",
    "    print('TRAINED MODEL NOT FOUND: Check that trained model is in eegdir and name matches: model_aied.pt')\n",
    "    \n",
    "###############################################\n",
    "\n",
    "### C: RUN MODEL\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for inputs,labels,paths in dataloaders[imgs]:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model.forward(inputs)\n",
    "        _,predicted = torch.max(outputs, 1) \n",
    "        pred = predicted.numpy()\n",
    "        lab = labels.numpy()\n",
    "        y_pred.append(pred)\n",
    "\n",
    "# reformat outputs:\n",
    "y_pred_flat = np.concatenate((y_pred),axis=0)\n",
    "# classes = ['class 0', 'class 1']\n",
    "df['predicted_class'] = y_pred_flat\n",
    "# # export as .csv\n",
    "# df.to_csv(proj_dir+'all_predictions.csv', encoding='utf-8', index=False)\n",
    "# print(df[:3]) # here, 1 = nonied, 0 = ied (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. CLEAN SPIKE DF FOR EXPORT:\n",
    "def dataCleaner(df, samp_freq = spikes.fs.values[0], win = 3): \n",
    "    \"\"\"\n",
    "    CLEANS SPIKE DATA FOR EXPORT\n",
    "    INPUT: df from resnet model, win = number of seconds allowed for spike overlap \n",
    "                                (i.e., spikes within 3s of ea.other = single event))\n",
    "    OUTPUT: clean df with subjectid, spikeStart, \n",
    "    channels (where spikes detected) - if contactName present, change to rownames, \n",
    "    numChannels (# channels spike detected)\n",
    "    \"\"\"\n",
    "    ### only keep spikes: predicted_class = 0\n",
    "    df = df[df.predicted_class == 0]\n",
    "    df['start'] = df['start'].astype(int) # convert from str to int\n",
    "    ### sort start times in df:\n",
    "    df = df.sort_values(by = 'start', ascending = True)\n",
    "    ### dedupe spikes by col and time\n",
    "    bins =  np.arange(min(df.start.values), max(df.start.values), samp_freq*win)\n",
    "    spikebins = np.digitize(df['start'], bins)\n",
    "    cleandf = df.groupby(spikebins)['start'].describe()\n",
    "    chanlist = df.groupby(spikebins)['chan'].apply(lambda x: x.values.tolist())\n",
    "    chanlist = [list(set(x)) for x in chanlist]\n",
    "    chancounts = [len(l) for l in chanlist]\n",
    "    meanspikestart = (cleandf['mean']).astype(int)\n",
    "    subjectid = [subject]*len(meanspikestart)\n",
    "    ### reformat into new df\n",
    "    finaldf = pd.DataFrame({'subject': subjectid, 'spike_start': meanspikestart, \n",
    "                            'channels_idx': chanlist, 'channels_count': chancounts})\n",
    "    ### reject spikes detected in >= 12 channels within time window\n",
    "    finaldf = finaldf[finaldf.channels_count < 12]\n",
    "    return (finaldf)\n",
    "\n",
    "finaldf = dataCleaner(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>spike_start</th>\n",
       "      <th>channels_idx</th>\n",
       "      <th>channels_names</th>\n",
       "      <th>channels_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mus21</td>\n",
       "      <td>294</td>\n",
       "      <td>[86, 32, 84, 134, 85, 33, 31]</td>\n",
       "      <td>[LFOA14, LTHB11, LFOA12, RFCA8, LFOA13, LTP2, ...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mus21</td>\n",
       "      <td>984</td>\n",
       "      <td>[49]</td>\n",
       "      <td>[LTF2]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mus21</td>\n",
       "      <td>1415</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[ LTA1, LTA2]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Mus21</td>\n",
       "      <td>5108</td>\n",
       "      <td>[40, 85, 31, 32]</td>\n",
       "      <td>[LTPH1, LFOA13, LTHB10, LTHB11]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Mus21</td>\n",
       "      <td>5894</td>\n",
       "      <td>[85, 86]</td>\n",
       "      <td>[LFOA13, LFOA14]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject  spike_start                   channels_idx  \\\n",
       "1    Mus21          294  [86, 32, 84, 134, 85, 33, 31]   \n",
       "2    Mus21          984                           [49]   \n",
       "3    Mus21         1415                         [0, 1]   \n",
       "9    Mus21         5108               [40, 85, 31, 32]   \n",
       "10   Mus21         5894                       [85, 86]   \n",
       "\n",
       "                                       channels_names  channels_count  \n",
       "1   [LFOA14, LTHB11, LFOA12, RFCA8, LFOA13, LTP2, ...               7  \n",
       "2                                              [LTF2]               1  \n",
       "3                                       [ LTA1, LTA2]               2  \n",
       "9                     [LTPH1, LFOA13, LTHB10, LTHB11]               4  \n",
       "10                                   [LFOA13, LFOA14]               2  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channels_df = pd.DataFrame({'channels': channels})\n",
    "channels_df.iloc[0][0] = channels_df.iloc[0][0].replace(\"#\", \"\")\n",
    "\n",
    "channels_names = []\n",
    "for index, row in finaldf.iterrows():\n",
    "    # grab the list of channel indices from \n",
    "    li = row[\"channels_idx\"]\n",
    "    channels_li = [channels_df.loc[int(i)].channels for i in li]\n",
    "    channels_names.append(channels_li)\n",
    "\n",
    "finaldf[\"channels_names\"] = channels_names\n",
    "finaldf = finaldf[[\"subject\", \"spike_start\", \"channels_idx\", \"channels_names\", \"channels_count\"]]\n",
    "finaldf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  subject  spike_start                   channels_idx  \\\n",
      "1   Mus21          294  [86, 32, 84, 134, 85, 33, 31]   \n",
      "2   Mus21          984                           [49]   \n",
      "3   Mus21         1415                         [0, 1]   \n",
      "\n",
      "                                      channels_names  channels_count  \n",
      "1  [LFOA14, LTHB11, LFOA12, RFCA8, LFOA13, LTP2, ...               7  \n",
      "2                                             [LTF2]               1  \n",
      "3                                      [ LTA1, LTA2]               2  \n",
      "\n",
      "FINAL SPIKES DETECTED =  606\n"
     ]
    }
   ],
   "source": [
    "today = datetime.today().strftime(\"%y%m%d\")\n",
    "finaldf.to_excel(f'{project_dir}/Data/Detected spikes/{subject}_run{run}_finalspikes_{today}.xlsx', index=False)\n",
    "print(finaldf[:3])\n",
    "print(\"\")\n",
    "print(\"FINAL SPIKES DETECTED = \", len(finaldf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
